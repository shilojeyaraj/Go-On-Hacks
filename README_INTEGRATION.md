# Complete Integration Guide - ToeGether with Gesture Recognition

## ğŸ¯ Overview

This application integrates:
- **Backend**: NestJS API with MongoDB and Firebase Auth
- **Frontend**: React app with gesture-controlled swiping
- **ML Model**: Trained gesture recognition model (YES/NO/NEUTRAL)

## ğŸš€ Quick Start

### 1. Install Dependencies

**Backend:**
```bash
cd backend
npm install
```

**Frontend:**
```bash
cd frontend
npm install
```

**Python (for ML predictions):**
```bash
# Activate virtual environment
source venv/bin/activate  # Windows: venv\Scripts\activate

# Verify packages
pip list | grep tensorflow
pip list | grep numpy
```

### 2. Configure Environment

**Backend `.env`** (copy from `backend/env.template`):
```env
PORT=3000
MONGODB_URI=mongodb://localhost:27017/ToeGether
FRONTEND_URL=http://localhost:3001
FB_PROJECT_ID=your-firebase-project-id
FB_CLIENT_EMAIL=your-firebase-client-email
FB_PRIVATE_KEY="your-firebase-private-key"
```

**Frontend `.env`** (copy from `frontend/env.example`):
```env
PORT=3001
REACT_APP_API_URL=http://localhost:3000
REACT_APP_FIREBASE_API_KEY=your-api-key
REACT_APP_FIREBASE_AUTH_DOMAIN=your-domain.firebaseapp.com
REACT_APP_FIREBASE_PROJECT_ID=your-project-id
REACT_APP_FIREBASE_STORAGE_BUCKET=your-bucket
REACT_APP_FIREBASE_MESSAGING_SENDER_ID=your-sender-id
REACT_APP_FIREBASE_APP_ID=your-app-id
```

### 3. Verify ML Model

```bash
# Check if model exists
ls ml-models/models/gesture_classifier.h5

# If missing, train it:
cd ml-models
source ../venv/bin/activate
python 2_extract_features.py
python 3_train_model.py
```

### 4. Start Services

**Terminal 1 - MongoDB (if local):**
```bash
mongod
```

**Terminal 2 - Backend:**
```bash
cd backend
npm run start:dev
```
âœ… Backend running on: `http://localhost:3000`

**Terminal 3 - Frontend:**
```bash
cd frontend
npm start
```
âœ… Frontend running on: `http://localhost:3001`

### 5. Use the Application

1. Open browser: `http://localhost:3001`
2. Sign up/Log in with Firebase
3. Navigate to **Swipe** page (`/swipe`)
4. Click **"Start Camera"**
5. Wait for buffer to fill (15 frames)
6. Perform gestures:
   - **Nod head up/down** â†’ YES gesture
   - **Shake head left/right** â†’ NO gesture
   - **Stay still** â†’ NEUTRAL

## ğŸ“ Project Structure

```
Go-On-Hacks/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”‚   â”œâ”€â”€ users/          # User management
â”‚   â”‚   â”‚   â””â”€â”€ gestures/       # Gesture recognition API âœ¨ NEW
â”‚   â”‚   â”œâ”€â”€ config/             # Firebase, env config
â”‚   â”‚   â””â”€â”€ main.ts             # App entry point
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env                    # Backend environment variables
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â””â”€â”€ Swipe/          # Gesture-controlled swipe page âœ¨ UPDATED
â”‚   â”‚   â”œâ”€â”€ components/         # Reusable components
â”‚   â”‚   â”œâ”€â”€ services/           # API services
â”‚   â”‚   â””â”€â”€ shared/             # Shared utilities
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env                    # Frontend environment variables
â”‚
â”œâ”€â”€ ml-models/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ gesture_classifier.h5  # Trained model
â”‚   â”œâ”€â”€ predict_gesture.py         # Prediction script âœ¨ NEW
â”‚   â”œâ”€â”€ 2_extract_features.py      # Feature extraction
â”‚   â””â”€â”€ 3_train_model.py           # Model training
â”‚
â””â”€â”€ venv/                       # Python virtual environment
```

## ğŸ”Œ API Endpoints

### Gesture Recognition

**POST `/gestures/predict`**
- **Auth**: Required (Firebase token)
- **Body**:
  ```json
  {
    "sequence": [
      [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],  // Frame 1 (9 features)
      ... // 15 frames total
    ]
  }
  ```
- **Response**:
  ```json
  {
    "success": true,
    "gesture": "YES",
    "confidence": 0.95,
    "probabilities": {
      "YES": 0.95,
      "NO": 0.03,
      "NEUTRAL": 0.02
    },
    "userId": "firebase-uid"
  }
  ```

**POST `/gestures/status`**
- **Auth**: Required
- **Response**:
  ```json
  {
    "modelLoaded": true
  }
  ```

### User Management

**GET `/users/profile`** - Get current user profile
**GET `/users/me`** - Get current user from database

## ğŸ¨ Frontend Features

### Swipe Page (`/swipe`)

- **Webcam Integration**: Real-time video capture
- **Face Detection**: MediaPipe face mesh
- **Feature Extraction**: 9 features per frame
- **Real-time Prediction**: Updates every 500ms
- **Visual Feedback**:
  - Buffer status bar
  - Gesture result with confidence
  - Color-coded gestures (Green=YES, Red=NO, Cyan=NEUTRAL)

## ğŸ”§ Troubleshooting

### Backend Issues

**"Model not found" error:**
```bash
# Check model exists
ls ml-models/models/gesture_classifier.h5

# Train if missing
cd ml-models
source ../venv/bin/activate
python 2_extract_features.py
python 3_train_model.py
```

**Python script errors:**
```bash
# Verify Python
python --version  # Should be 3.8+

# Check packages
pip list | grep tensorflow
pip list | grep numpy

# Install if missing
pip install tensorflow numpy
```

**MongoDB connection:**
```bash
# Check MongoDB is running
mongod --version

# Or use MongoDB Atlas connection string in .env
MONGODB_URI=mongodb+srv://...
```

### Frontend Issues

**Webcam not working:**
- Grant browser camera permissions
- Use HTTPS in production (required for webcam)
- Check browser console for errors

**MediaPipe not loading:**
- Requires internet connection (CDN)
- Check browser console
- Verify script loads in Network tab

**API connection errors:**
- Verify backend is running on port 3000
- Check `REACT_APP_API_URL` in `.env`
- Check CORS settings in backend

### Performance Issues

**Slow predictions:**
- Reduce prediction interval in `Swipe.tsx`
- Optimize model (reduce sequence length)
- Use GPU for TensorFlow (if available)

**High CPU usage:**
- Reduce video resolution
- Increase prediction interval
- Optimize MediaPipe settings

## ğŸ“ Development Workflow

### Making Changes

1. **Backend changes**: Auto-reloads with `npm run start:dev`
2. **Frontend changes**: Hot-reloads automatically
3. **ML model changes**: Retrain and restart backend

### Testing

```bash
# Backend tests
cd backend
npm test

# Frontend tests
cd frontend
npm test

# Test gesture prediction directly
cd ml-models
python predict_gesture.py '[[0.5]*9]*15'
```

## ğŸš¢ Production Deployment

### Backend

```bash
cd backend
npm run build
npm run start:prod
```

### Frontend

```bash
cd frontend
npm run build
# Serve build/ directory with nginx/Apache
```

### Environment Variables

Update production `.env` files with:
- Production MongoDB URI
- Production Firebase credentials
- Production frontend URL
- HTTPS URLs for webcam access

## ğŸ“š Additional Resources

- **Setup Guide**: See `SETUP_GUIDE.md` for detailed setup
- **Quick Start**: See `QUICK_START.md` for quick reference
- **Integration Details**: See `INTEGRATION_SUMMARY.md` for architecture

## ğŸ¯ Next Steps

1. **Add Swipe Actions**: Map gestures to swipe left/right
2. **Gesture History**: Store and display gesture statistics
3. **UI Improvements**: Add animations and better feedback
4. **Performance**: Optimize model and reduce latency
5. **Testing**: Add unit and integration tests

## ğŸ’¡ Tips

- Keep backend and frontend running in separate terminals
- Check browser console for frontend errors
- Check backend terminal for API errors
- Use browser DevTools Network tab to debug API calls
- Test gesture prediction script directly before testing in app

## ğŸ†˜ Getting Help

1. Check logs (backend terminal, browser console)
2. Verify all environment variables are set
3. Ensure all services are running
4. Check model file exists and is valid
5. Verify Python is accessible from backend

---

**Happy Gesturing! ğŸ‰**

